{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Collaboration (GPT + Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_Report = {\n",
    "  \"GPT\": {\n",
    "    \"History\": \"continuity and change over time\",\n",
    "    \"US_Government\": \"Analyzing evidence\",\n",
    "    \"Physics\": \"Analyzing evidence\",\n",
    "    \"Human_Geography\": \"Contextualization\",\n",
    "    \"Environmental_Science\": \"Analyzing evidence\"\n",
    "  },\n",
    "  \"Gemini\": {\n",
    "    \"History\": \"Continuity and change over time\",\n",
    "    \"US_Government\": \"Analyzing evidence\",\n",
    "    \"Physics\": \"Causation\",\n",
    "    \"Human_Geography\": \"Causation\",\n",
    "    \"Environmental_Science\": \"Analyzing evidence\"\n",
    "  },\n",
    "  \"Claude\": {\n",
    "    \"History\": \"Analyzing evidence\",\n",
    "    \"US_Government\": \"Contextualization\",\n",
    "    \"Physics\": \"Analyzing evidence\",\n",
    "    \"Human_Geography\": \"Causation\",\n",
    "    \"Environmental_Science\": \"Analyzing evidence\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "Claude_client = anthropic.Anthropic(api_key=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Test (question):\n",
    "#### History Test = question[1]- question[55]\n",
    "#### Government Test = question[1]- question[96]\n",
    "#### Physics Test = question[1]- question[75]\n",
    "#### Human Geo Test = question[1]- question[105]\n",
    "#### Env Sci Test = question[1]- question[157]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"history\" # Please update to correct Subject\n",
    "def load_questions_from_folder(folder_path):\n",
    "    question = {}\n",
    "    for i in range(1, 56):  # add 1 more to the last question number to include it!\n",
    "        file_name = f\"Question {i}.txt\" #Add space between 'Question' and question number for history\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                question[i] = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='cp1252') as file:\n",
    "                question[i] = file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue  \n",
    "\n",
    "    return question\n",
    "\n",
    "folder_path = \"\"\n",
    "question = load_questions_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answers_path = \"\" #update here\n",
    "try:\n",
    "    with open(Answers_path, 'r', encoding='utf-8') as file:\n",
    "        history_answers = file.read() #update here\n",
    "except UnicodeDecodeError:\n",
    "    with open(Answers_path, 'r', encoding='cp1252') as file:  \n",
    "        history_answers = file.read() #update here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Round_1_GPT_answer = {}\n",
    "\n",
    "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
    "    output = client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n\"}],\n",
    "        )\n",
    "    Round_1_GPT_answer[i] = output.choices[0].message.content\n",
    "    print(output.choices[0].message.content)\n",
    "\n",
    "print(\"All questions processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview GPT round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Round_1_GPT_answer[55]) #preview an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_temperature_setting = genai.GenerationConfig(temperature= 0.1)\n",
    "no_safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "Round_1_Gemini_answer = {}\n",
    "\n",
    "i = 1  # Start from the first question\n",
    "\n",
    "while i <= 55:  # change based on the ACTUAL last question\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            contents=f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct choice to 100% being the most likely correct answer. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n\",\n",
    "            safety_settings= no_safety_settings\n",
    "        )\n",
    "        Round_1_Gemini_answer[i] = response.text\n",
    "        print(response.text)\n",
    "        i += 1  # Move to the next question only if successful\n",
    "    except Exception as e:  # Catching a general exception\n",
    "        error_message = str(e)\n",
    "        if \"429\" in error_message or \"rate limit\" in error_message.lower():\n",
    "            print(f\"Rate limit error on question {i}: {error_message} - Sleeping longer and retrying...\")\n",
    "            time.sleep(4)  # Sleep longer if rate limit error occurs\n",
    "        else:\n",
    "            print(f\"Error on question {i}: {error_message} - Retrying...\")\n",
    "    finally:\n",
    "        time.sleep(3)  # Respect the API's request limit\n",
    "\n",
    "print(\"All questions processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Gemini Round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Round_1_Gemini_answer[55]) #preview an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1 CSV:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agreement: (Don't Download; still run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_agreement(GPT_answers, Gemini_answers):\n",
    "    results = []\n",
    "    agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
    "\n",
    "    for question_number in range(1, 56):\n",
    "        gpt_answer_text = GPT_answers[question_number]\n",
    "        gemini_answer_text = Gemini_answers[question_number]\n",
    "\n",
    "        # Extracting the first letter choice mentioned in their responses\n",
    "        gpt_match = agreement_pattern.search(gpt_answer_text)\n",
    "        gemini_match = agreement_pattern.search(gemini_answer_text)\n",
    "\n",
    "        if gpt_match and gemini_match:\n",
    "            # Compare the first letter found in each response\n",
    "            gpt_choice = gpt_match.group().upper()\n",
    "            gemini_choice = gemini_match.group().upper()\n",
    "            correctness = 1 if gpt_choice == gemini_choice else 0\n",
    "        else:\n",
    "            # If either response does not contain a valid letter choice\n",
    "            correctness = 0\n",
    "\n",
    "        results.append((question_number, correctness))\n",
    "\n",
    "    return results\n",
    "\n",
    "results = evaluate_llm_agreement(Round_1_GPT_answer, Round_1_Gemini_answer)\n",
    "\n",
    "def initialize_csv_file(filename):\n",
    "    \"\"\"Initialize the CSV file with headers.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Question Number\", \"Agreement (0/1)\", \"GPT Choice\", \"Gemini Choice\"])\n",
    "\n",
    "def update_csv(filename, results, GPT_answers, Gemini_answers):\n",
    "    \"\"\"Update the CSV file with LLM agreement results.\"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
    "\n",
    "        for question_number, correctness in results:\n",
    "            gpt_match = agreement_pattern.search(GPT_answers[question_number])\n",
    "            gemini_match = agreement_pattern.search(Gemini_answers[question_number])\n",
    "            gpt_choice = gpt_match.group().upper() if gpt_match else \"N/A\"\n",
    "            gemini_choice = gemini_match.group().upper() if gemini_match else \"N/A\"\n",
    "\n",
    "            writer.writerow([question_number, correctness, gpt_choice, gemini_choice])\n",
    "\n",
    "# Example usage\n",
    "filename = f\"All3_round_1_{subject}_Agree.csv\"\n",
    "initialize_csv_file(filename)\n",
    "update_csv(filename, results, Round_1_GPT_answer, Round_1_Gemini_answer)\n",
    "print(f'Agreement CSV: {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grader: (Put correctness in Master XLSX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_key(answers):\n",
    "    answer_key = {}\n",
    "    for line in answers.splitlines():\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
    "            answer_key[question_number] = (correct_answer, skill)\n",
    "    return answer_key\n",
    "\n",
    "answers = history_answers # Please update \n",
    "\n",
    "def grade_llm_answers(csv_filename, answer_key):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    # Add a new column for the truth (correct answer from the answer key)\n",
    "    df['Truth'] = \"\"\n",
    "\n",
    "    # Grading each response\n",
    "    for index, row in df.iterrows():\n",
    "        question_number = str(row['Question Number'])\n",
    "        # Retrieve the correct answer and skill from the answer key\n",
    "        if question_number in answer_key:\n",
    "            correct_answer, skill_assessed = answer_key[question_number]\n",
    "            df.at[index, 'Truth'] = correct_answer\n",
    "            df.at[index, 'Skill Assessed'] = skill_assessed\n",
    "\n",
    "            gpt_choice = extract_choice(str(row['GPT Choice']))\n",
    "            gemini_choice = extract_choice(str(row['Gemini Choice']))\n",
    "\n",
    "            # If GPT and Gemini choices agree and are valid choices\n",
    "            if gpt_choice and gemini_choice and gpt_choice == gemini_choice:\n",
    "                df.at[index, 'Correctness'] = int(gpt_choice == correct_answer)\n",
    "            else:\n",
    "                # If either LLM choice is invalid or they disagree\n",
    "                df.at[index, 'Correctness'] = 0\n",
    "        else:\n",
    "            df.at[index, 'Correctness'] = 0\n",
    "            df.at[index, 'Skill Assessed'] = \"Question not in answer key\"\n",
    "\n",
    "    # Save the graded DataFrame back to a new CSV file\n",
    "    graded_filename = csv_filename.replace('Agree.csv', 'graded.csv')\n",
    "    df.to_csv(graded_filename, index=False)\n",
    "    print(f\"Graded CSV: {graded_filename}\")\n",
    "\n",
    "def extract_choice(choice_text):\n",
    "    # Extract letter choice from strings like \"1. C\" or \"5. C\"\n",
    "    match = re.match(r'\\d+\\.\\s*([A-E])', choice_text, re.IGNORECASE)\n",
    "    return match.group(1).upper() if match else None\n",
    "\n",
    "answer_key = parse_answer_key(answers)\n",
    "csv_filename = filename # Round_1_results\n",
    "grade_llm_answers(csv_filename, answer_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Round_2_GPT_answer = {}\n",
    "\n",
    "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
    "    output = client.chat.completions.create(\n",
    "            model='gpt-4',\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n based on Gemini's reponse:'{Round_1_Gemini_answer[i]}' \\nand your own previous response:'{Round_1_GPT_answer[i]}'. \\nLimit your choice to one of the previous responses\"}],\n",
    "        )\n",
    "    Round_2_GPT_answer[i] = output.choices[0].message.content\n",
    "    print(output.choices[0].message.content)\n",
    "\n",
    "print(\"All questions processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Round_2_GPT_answer[55]) #preview an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "Round_2_Gemini_answer = {}\n",
    "\n",
    "i = 1  # Start from the first question\n",
    "\n",
    "while i <= 55:  # change based on last question\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            contents=f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n based on your previous reponse:'{Round_1_Gemini_answer[i]}' \\nand GPT's response:'{Round_1_GPT_answer[i]}'. \\nLimit your choice to one of the previous responses\",\n",
    "            safety_settings= no_safety_settings\n",
    "        )\n",
    "        Round_2_Gemini_answer[i] = response.text\n",
    "        print(response.text)\n",
    "        i += 1 \n",
    "    except Exception as e:  # Catching a general exception\n",
    "        error_message = str(e)\n",
    "        if \"429\" in error_message or \"rate limit\" in error_message.lower():\n",
    "            print(f\"Rate limit error on question {i}: {error_message} - Sleeping longer and retrying...\")\n",
    "            time.sleep(4)  # Sleep longer if rate limit error occurs\n",
    "        else:\n",
    "            print(f\"Error on question {i}: {error_message} - Retrying...\")\n",
    "    finally:\n",
    "        time.sleep(3)  # Respect the API's request limit\n",
    "\n",
    "print(\"All questions processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Round_2_Gemini_answer[55]) #preview an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2 CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_agreement(GPT_answers, Gemini_answers):\n",
    "    results = []\n",
    "    agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
    "\n",
    "    for question_number in range(1, 56): # Add 1 more to the last question number to include it\n",
    "        gpt_answer_text = GPT_answers[question_number]\n",
    "        gemini_answer_text = Gemini_answers[question_number]\n",
    "\n",
    "        # Extracting the first letter choice mentioned in their responses\n",
    "        gpt_match = agreement_pattern.search(gpt_answer_text)\n",
    "        gemini_match = agreement_pattern.search(gemini_answer_text)\n",
    "\n",
    "        if gpt_match and gemini_match:\n",
    "            # Compare the first letter found in each response\n",
    "            gpt_choice = gpt_match.group().upper()\n",
    "            gemini_choice = gemini_match.group().upper()\n",
    "            correctness = 1 if gpt_choice == gemini_choice else 0\n",
    "        else:\n",
    "            # If either response does not contain a valid letter choice\n",
    "            correctness = 0\n",
    "\n",
    "        results.append((question_number, correctness))\n",
    "\n",
    "    return results\n",
    "\n",
    "results = evaluate_llm_agreement(Round_2_GPT_answer, Round_2_Gemini_answer)\n",
    "\n",
    "def initialize_csv_file(filename):\n",
    "    \"\"\"Initialize the CSV file with headers.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Question Number\", \"Agreement (0/1)\", \"GPT Choice\", \"Gemini Choice\"])\n",
    "\n",
    "def update_csv(filename, results, GPT_answers, Gemini_answers):\n",
    "    \"\"\"Update the CSV file with LLM agreement results.\"\"\"\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
    "\n",
    "        for question_number, correctness in results:\n",
    "            gpt_match = agreement_pattern.search(GPT_answers[question_number])\n",
    "            gemini_match = agreement_pattern.search(Gemini_answers[question_number])\n",
    "            gpt_choice = gpt_match.group().upper() if gpt_match else \"N/A\"\n",
    "            gemini_choice = gemini_match.group().upper() if gemini_match else \"N/A\"\n",
    "\n",
    "            writer.writerow([question_number, correctness, gpt_choice, gemini_choice])\n",
    "\n",
    "# Example usage\n",
    "filename = f\"GPT_Gemini_round_2_{subject}_Agree.csv\"\n",
    "initialize_csv_file(filename)\n",
    "update_csv(filename, results, Round_2_GPT_answer, Round_2_Gemini_answer)\n",
    "print(f'Agreement CSV: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer_key(answers):\n",
    "    answer_key = {}\n",
    "    for line in answers.splitlines():\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
    "            answer_key[question_number] = (correct_answer, skill)\n",
    "    return answer_key\n",
    "\n",
    "answers = history_answers # Please update \n",
    "\n",
    "def grade_llm_answers(csv_filename, answer_key):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    # Add a new column for the truth (correct answer from the answer key)\n",
    "    df['Truth'] = \"\"\n",
    "\n",
    "    # Grading each response\n",
    "    for index, row in df.iterrows():\n",
    "        question_number = str(row['Question Number'])\n",
    "        # Retrieve the correct answer and skill from the answer key\n",
    "        if question_number in answer_key:\n",
    "            correct_answer, skill_assessed = answer_key[question_number]\n",
    "            df.at[index, 'Truth'] = correct_answer\n",
    "            df.at[index, 'Skill Assessed'] = skill_assessed\n",
    "\n",
    "            gpt_choice = extract_choice(str(row['GPT Choice']))\n",
    "            gemini_choice = extract_choice(str(row['Gemini Choice']))\n",
    "\n",
    "            # If GPT and Gemini choices agree and are valid choices\n",
    "            if gpt_choice and gemini_choice and gpt_choice == gemini_choice:\n",
    "                df.at[index, 'Correctness'] = int(gpt_choice == correct_answer)\n",
    "            else:\n",
    "                # If either LLM choice is invalid or they disagree\n",
    "                df.at[index, 'Correctness'] = 0\n",
    "        else:\n",
    "            df.at[index, 'Correctness'] = 0\n",
    "            df.at[index, 'Skill Assessed'] = \"Question not in answer key\"\n",
    "\n",
    "    # Save the graded DataFrame back to a new CSV file\n",
    "    graded_filename = csv_filename.replace('Agree.csv', 'graded.csv')\n",
    "    df.to_csv(graded_filename, index=False)\n",
    "    print(f\"Graded CSV: {graded_filename}\")\n",
    "\n",
    "def extract_choice(choice_text):\n",
    "    # Extract letter choice from strings like \"1. C\" or \"5. C\"\n",
    "    match = re.match(r'\\d+\\.\\s*([A-E])', choice_text, re.IGNORECASE)\n",
    "    return match.group(1).upper() if match else None\n",
    "\n",
    "answer_key = parse_answer_key(answers)\n",
    "csv_filename = filename # Round_2_results\n",
    "grade_llm_answers(csv_filename, answer_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skills_assessed(answers):\n",
    "    skills_assessed = {}\n",
    "    for line in answers.splitlines():\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            question_number = parts[0]\n",
    "            skill = ' '.join(parts[2:])\n",
    "            skills_assessed[question_number] = skill\n",
    "    return skills_assessed\n",
    "\n",
    "skills_assessed = parse_skills_assessed(answers)\n",
    "\n",
    "\n",
    "def parse_answer_key(answers):\n",
    "    answer_key = {}\n",
    "    for line in answers.splitlines():\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 3:\n",
    "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
    "            answer_key[question_number] = (correct_answer, skill)\n",
    "    return answer_key\n",
    "\n",
    "def extract_letter_choice(text):\n",
    "    match = re.search(r'\\b([A-E])\\b', text, re.IGNORECASE)\n",
    "    return match.group(1).upper() if match else None\n",
    "\n",
    "def initialize_csv_file(filename, subject):\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([f\"{subject}_Question Number\", f\"{subject}_Correctness\", f\"{subject}_Choice\", f\"{subject}_Skill Assessed\"])\n",
    "\n",
    "def update_csv(filename, answers, subject, answer_key):\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(1, 56):  # Add 1 more to the last question number to include it\n",
    "            answer_text = answers[i] if i in answers else \"\"\n",
    "            choice = extract_letter_choice(answer_text)\n",
    "            correct = 1 if answer_key.get(str(i), ('', ''))[0] == choice else 0\n",
    "            skill_assessed = answer_key.get(str(i), ('', ''))[1]\n",
    "            writer.writerow([f\"{subject}_{i}\", correct, choice, skill_assessed])\n",
    "\n",
    "def ask_claude_all_questions(subject, filename, answer_key):\n",
    "    initialize_csv_file(filename, subject)  \n",
    "    claude_answers = {}  \n",
    "\n",
    "    for i in range(1, 56):  # Add 1 more to the last question number to include it\n",
    "        message = Claude_client.messages.create(\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with only the question number and your letter of choice. Please remove parentheses from your letter of choice. Please answer according to the example format: '1. A' Now, please answer question {i}: \\n{question[i]}\\n based on Gemini's reponse:'{Round_2_Gemini_answer[i]}' \\nand GPT's response:'{Round_2_GPT_answer[i]}'. The percentage provided for their responses is their confidence level from 0% being least likely to be correct and 100% being most likelt to be the correct answer. This question assess the skill of: {skills_assessed[str(i)]} \\nYou are the judge and you can decide to choose which LLM's choice you want to choose. You can also decide to choose neither of their choices. Also decide based on each LLM's weaknesses:{LLM_Report}\"}],\n",
    "            model=\"claude-3-opus-20240229\"\n",
    "        )\n",
    "        claude_answers[i] = message.content[0].text \n",
    "        print(f\"Question {i}: {claude_answers[i]}\")\n",
    "\n",
    "    update_csv(filename, claude_answers, subject, answer_key) \n",
    "    print(f\"Round 3 finalized. Results added to {filename}.\")\n",
    "\n",
    "filename = f\"Claude_Round_3_{subject}_Round_3_Results.csv\"\n",
    "answers = history_answers  \n",
    "answer_key = parse_answer_key(answers)\n",
    "ask_claude_all_questions(subject, filename, answer_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
