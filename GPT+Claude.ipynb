{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmnTzE_e__zC"
      },
      "source": [
        "# LLM Collaboration (GPT & Claude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neYliB9S_9hv",
        "outputId": "02cd2c28-2527-4bc7-db89-2571963196aa"
      },
      "outputs": [],
      "source": [
        "!pip install -q OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtuat1AgASCf"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8v5TQ-VAURH",
        "outputId": "bc109b21-f85a-445a-d8e9-56d39000457d"
      },
      "outputs": [],
      "source": [
        "!pip install -q anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPm6qZirAVr8"
      },
      "outputs": [],
      "source": [
        "LLM_Report = {\n",
        "  \"GPT\": {\n",
        "    \"History\": \"continuity and change over time\",\n",
        "    \"US_Government\": \"Analyzing evidence\",\n",
        "    \"Physics\": \"Analyzing evidence\",\n",
        "    \"Human_Geography\": \"Contextualization\",\n",
        "    \"Environmental_Science\": \"Analyzing evidence\"\n",
        "  },\n",
        "  \"Gemini\": {\n",
        "    \"History\": \"Continuity and change over time\",\n",
        "    \"US_Government\": \"Analyzing evidence\",\n",
        "    \"Physics\": \"Causation\",\n",
        "    \"Human_Geography\": \"Causation\",\n",
        "    \"Environmental_Science\": \"Analyzing evidence\"\n",
        "  },\n",
        "  \"Claude\": {\n",
        "    \"History\": \"Analyzing evidence\",\n",
        "    \"US_Government\": \"Contextualization\",\n",
        "    \"Physics\": \"Analyzing evidence\",\n",
        "    \"Human_Geography\": \"Causation\",\n",
        "    \"Environmental_Science\": \"Analyzing evidence\"\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRMrv-yPAXVl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6us3FMpAZjm"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0z4zKy2AcjW"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "genai.configure(api_key=\"\")\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7ilYc_aAeIC"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "Claude_client = anthropic.Anthropic(api_key=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxK7bw-pAhNI"
      },
      "source": [
        "# Import Test (question)\n",
        "### History Test = question[1]-question[55]\n",
        "### Government Test = question[1]-question[96]\n",
        "### Physics Test = question[1]-question[75]\n",
        "### Human Geo Test = question[1]-question[105]\n",
        "### Env Sci Test = question[1]-question[157]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGojIFIaBBR9"
      },
      "outputs": [],
      "source": [
        "subject = \"US History\" # Please update to correct Subject\n",
        "def load_questions_from_folder(folder_path):\n",
        "    question = {}\n",
        "    for i in range(1, 56):  # add 1 more to the last question number to include it!\n",
        "        file_name = f\"Question {i}.txt\" #Add space between 'Question' and question number for history\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                question[i] = file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='cp1252') as file:\n",
        "                question[i] = file.read()\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "            continue\n",
        "\n",
        "    return question\n",
        "\n",
        "folder_path = \"/content/\" # Keep as it\n",
        "question = load_questions_from_folder(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXGhEj_HBS3X"
      },
      "source": [
        "# Import Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUR8xlZKBRWZ"
      },
      "outputs": [],
      "source": [
        "Answers_path = \"\" #update here\n",
        "try:\n",
        "    with open(Answers_path, 'r', encoding='utf-8') as file:\n",
        "        ushistory_answers = file.read() #update here\n",
        "except UnicodeDecodeError:\n",
        "    with open(Answers_path, 'r', encoding='cp1252') as file:\n",
        "        ushistory_answers = file.read() #update here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aycOmX37BZOR"
      },
      "source": [
        "# Round 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8TRhLbzBbLF"
      },
      "source": [
        "## GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMdENvIBYUv",
        "outputId": "948aa87a-c2d3-4543-cb27-4221057b4a71"
      },
      "outputs": [],
      "source": [
        "Round_1_GPT_answer = {}\n",
        "\n",
        "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
        "    output = client.chat.completions.create(\n",
        "            model='gpt-4',\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n\"}],\n",
        "        )\n",
        "    Round_1_GPT_answer[i] = output.choices[0].message.content\n",
        "    print(output.choices[0].message.content)\n",
        "\n",
        "print(\"All questions processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyR85BGNBiSi"
      },
      "source": [
        "## Preview GPT round 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cveuAa3rBhpQ",
        "outputId": "23a9f134-5b7a-4716-dfb3-7203925bfb2f"
      },
      "outputs": [],
      "source": [
        "print(Round_1_GPT_answer[54]) #preview an answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2J2EtABpE3"
      },
      "source": [
        "## Claude 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWF7LwU1BsON",
        "outputId": "c173f670-d12e-4b33-cf46-70a6855c89ca"
      },
      "outputs": [],
      "source": [
        "Round_1_Claude_answer = {}\n",
        "\n",
        "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
        "    message = Claude_client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n\"}],\n",
        "        )\n",
        "    Round_1_Claude_answer[i] = message.content[0].text\n",
        "    print(message.content[0].text)\n",
        "\n",
        "print(\"All questions processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4FkJFWLB3Bq"
      },
      "source": [
        "## Preview Claude round 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiKzI4zsB6zi"
      },
      "outputs": [],
      "source": [
        "print(Round_1_Claude_answer[55]) #preview an answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4BR7RWyCDaO"
      },
      "source": [
        "# Round 1 CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kmtGMikCFYK",
        "outputId": "3019004e-6b0a-4dfd-af5a-edf3ac2caecb"
      },
      "outputs": [],
      "source": [
        "def evaluate_llm_agreement(GPT_answers, Claude_answers):\n",
        "    results = []\n",
        "    agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
        "\n",
        "    for question_number in range(1, 56):\n",
        "        gpt_answer_text = GPT_answers[question_number]\n",
        "        claude_answer_text = Claude_answers[question_number]\n",
        "\n",
        "        # Extracting the first letter choice mentioned in their responses\n",
        "        gpt_match = agreement_pattern.search(gpt_answer_text)\n",
        "        claude_match = agreement_pattern.search(claude_answer_text)\n",
        "\n",
        "        if gpt_match and claude_match:\n",
        "            # Compare the first letter found in each response\n",
        "            gpt_choice = gpt_match.group().upper()\n",
        "            claude_choice = claude_match.group().upper()\n",
        "            correctness = 1 if gpt_choice == claude_choice else 0\n",
        "        else:\n",
        "            # If either response does not contain a valid letter choice\n",
        "            correctness = 0\n",
        "\n",
        "        results.append((question_number, correctness))\n",
        "\n",
        "    return results\n",
        "\n",
        "results = evaluate_llm_agreement(Round_1_GPT_answer, Round_1_Claude_answer)\n",
        "\n",
        "def initialize_csv_file(filename):\n",
        "    \"\"\"Initialize the CSV file with headers.\"\"\"\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Question Number\", \"Agreement (0/1)\", \"GPT Choice\", \"Claude Choice\"])\n",
        "\n",
        "def update_csv(filename, results, GPT_answers, Claude_answers):\n",
        "    \"\"\"Update the CSV file with LLM agreement results.\"\"\"\n",
        "    with open(filename, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
        "\n",
        "        for question_number, correctness in results:\n",
        "            gpt_match = agreement_pattern.search(GPT_answers[question_number])\n",
        "            claude_match = agreement_pattern.search(Claude_answers[question_number])\n",
        "            gpt_choice = gpt_match.group().upper() if gpt_match else \"N/A\"\n",
        "            claude_choice = claude_match.group().upper() if claude_match else \"N/A\"\n",
        "\n",
        "            writer.writerow([question_number, correctness, gpt_choice, claude_choice])\n",
        "\n",
        "filename = f\"GPT_Claude_round_1_{subject}_Agree.csv\"\n",
        "initialize_csv_file(filename)\n",
        "update_csv(filename, results, Round_1_GPT_answer, Round_1_Claude_answer)\n",
        "print(f'Agreement CSV: {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q__nRxB6DLck",
        "outputId": "4697a74d-6042-417d-a46d-d751946eec7f"
      },
      "outputs": [],
      "source": [
        "def parse_answer_key(answers):\n",
        "    answer_key = {}\n",
        "    for line in answers.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 3:\n",
        "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
        "            answer_key[question_number] = (correct_answer, skill)\n",
        "    return answer_key\n",
        "\n",
        "answers = ushistory_answers # Please update\n",
        "\n",
        "def grade_llm_answers(csv_filename, answer_key):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    # Add a new column for the truth (correct answer from the answer key)\n",
        "    df['Truth'] = \"\"\n",
        "\n",
        "    # Grading each response\n",
        "    for index, row in df.iterrows():\n",
        "        question_number = str(row['Question Number'])\n",
        "        # Retrieve the correct answer and skill from the answer key\n",
        "        if question_number in answer_key:\n",
        "            correct_answer, skill_assessed = answer_key[question_number]\n",
        "            df.at[index, 'Truth'] = correct_answer\n",
        "            df.at[index, 'Skill Assessed'] = skill_assessed\n",
        "\n",
        "            gpt_choice = extract_choice(str(row['GPT Choice']))\n",
        "            claude_choice = extract_choice(str(row['Claude Choice']))\n",
        "\n",
        "            # If GPT and Claude choices agree and are valid choices\n",
        "            if gpt_choice and claude_choice and gpt_choice == claude_choice:\n",
        "                df.at[index, 'Correctness'] = int(gpt_choice == correct_answer)\n",
        "            else:\n",
        "                # If either LLM choice is invalid or they disagree\n",
        "                df.at[index, 'Correctness'] = 0\n",
        "        else:\n",
        "            df.at[index, 'Correctness'] = 0\n",
        "            df.at[index, 'Skill Assessed'] = \"Question not in answer key\"\n",
        "\n",
        "    # Save the graded DataFrame back to a new CSV file\n",
        "    graded_filename = csv_filename.replace('Agree.csv', 'graded.csv')\n",
        "    df.to_csv(graded_filename, index=False)\n",
        "    print(f\"Graded CSV: {graded_filename}\")\n",
        "\n",
        "def extract_choice(choice_text):\n",
        "    # Extract letter choice from strings like \"1. C\" or \"5. C\"\n",
        "    match = re.match(r'\\d+\\.\\s*([A-E])', choice_text, re.IGNORECASE)\n",
        "    return match.group(1).upper() if match else None\n",
        "\n",
        "answer_key = parse_answer_key(answers)\n",
        "csv_filename = filename # Round_1_results\n",
        "grade_llm_answers(csv_filename, answer_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvmaB1p0DjKA"
      },
      "source": [
        "# Round 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_x2Lb7kDlEg",
        "outputId": "a2bf6342-243f-457b-97ca-b41b0f159768"
      },
      "outputs": [],
      "source": [
        "Round_2_GPT_answer = {}\n",
        "\n",
        "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
        "    output = client.chat.completions.create(\n",
        "            model='gpt-4',\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n based on Claude's reponse:'{Round_1_Claude_answer[i]}' \\nand your own previous response:'{Round_1_GPT_answer[i]}'. \\nLimit your choice to one of the previous responses\"}],\n",
        "        )\n",
        "    Round_2_GPT_answer[i] = output.choices[0].message.content\n",
        "    print(output.choices[0].message.content)\n",
        "\n",
        "print(\"All questions processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6PnFdQRDzEd"
      },
      "outputs": [],
      "source": [
        "print(Round_2_GPT_answer[55]) #preview an answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0hqe4jD5gW",
        "outputId": "f4670a46-cfd4-4e52-86be-9f00da80ebd9"
      },
      "outputs": [],
      "source": [
        "Round_2_Claude_answer = {}\n",
        "\n",
        "for i in range(1, 56): # Add 1 more to the last question number to include it\n",
        "    message = Claude_client.messages.create(\n",
        "            max_tokens=1000,\n",
        "            model=\"claude-3-opus-20240229\",\n",
        "            messages=[{\"role\": \"user\", \"content\": f\"Please answer the questions with the question number and your letter of choice. Please remove parentheses from your letter of choice. Please provide a confidence level for your letter choice from 0% being the least likely correct answer choice to 100% being the most likely correct answer choice. Please also include a 1 sentence explanation for your choice justification. Please answer according to the example format: '1. A 90% because...' Now, please answer question {i}: \\n{question[i]}\\n based on your previous reponse:'{Round_1_Claude_answer[i]}' \\nand GPT's response:'{Round_1_GPT_answer[i]}'. \\nLimit your choice to one of the previous responses\"}],\n",
        "        )\n",
        "    Round_2_Claude_answer[i] = message.content[0].text\n",
        "    print(message.content[0].text)\n",
        "\n",
        "print(\"All questions processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtV7OZBZETyC"
      },
      "source": [
        "## Round 2 CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgiPBk5FEWDQ",
        "outputId": "c416b89c-ef86-4647-f237-df24bb4dd2f6"
      },
      "outputs": [],
      "source": [
        "def evaluate_llm_agreement(GPT_answers, Claude_answers):\n",
        "    results = []\n",
        "    agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
        "\n",
        "    for question_number in range(1, 56): # Add 1 more to the last question number to include it\n",
        "        gpt_answer_text = GPT_answers[question_number]\n",
        "        claude_answer_text = Claude_answers[question_number]\n",
        "\n",
        "        # Extracting the first letter choice mentioned in their responses\n",
        "        gpt_match = agreement_pattern.search(gpt_answer_text)\n",
        "        claude_match = agreement_pattern.search(claude_answer_text)\n",
        "\n",
        "        if gpt_match and claude_match:\n",
        "            # Compare the first letter found in each response\n",
        "            gpt_choice = gpt_match.group().upper()\n",
        "            claude_choice = claude_match.group().upper()\n",
        "            correctness = 1 if gpt_choice == claude_choice else 0\n",
        "        else:\n",
        "            # If either response does not contain a valid letter choice\n",
        "            correctness = 0\n",
        "\n",
        "        results.append((question_number, correctness))\n",
        "\n",
        "    return results\n",
        "\n",
        "results = evaluate_llm_agreement(Round_2_GPT_answer, Round_2_Claude_answer)\n",
        "\n",
        "def initialize_csv_file(filename):\n",
        "    \"\"\"Initialize the CSV file with headers.\"\"\"\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Question Number\", \"Agreement (0/1)\", \"GPT Choice\", \"Claude Choice\"])\n",
        "\n",
        "def update_csv(filename, results, GPT_answers, Claude_answers):\n",
        "    \"\"\"Update the CSV file with LLM agreement results.\"\"\"\n",
        "    with open(filename, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        agreement_pattern = re.compile(r'(\\d+)\\s*[.:]?\\s*(?:\\(([A-E])\\)|([A-E]))', re.IGNORECASE)\n",
        "\n",
        "        for question_number, correctness in results:\n",
        "            gpt_match = agreement_pattern.search(GPT_answers[question_number])\n",
        "            claude_match = agreement_pattern.search(Claude_answers[question_number])\n",
        "            gpt_choice = gpt_match.group().upper() if gpt_match else \"N/A\"\n",
        "            claude_choice = claude_match.group().upper() if claude_match else \"N/A\"\n",
        "\n",
        "            writer.writerow([question_number, correctness, gpt_choice, claude_choice])\n",
        "\n",
        "filename = f\"GPT_Claude_round_2_{subject}_Agree.csv\"\n",
        "initialize_csv_file(filename)\n",
        "update_csv(filename, results, Round_2_GPT_answer, Round_2_Claude_answer)\n",
        "print(f'Agreement CSV: {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwLks16CE3UB",
        "outputId": "02d5d4f4-1d14-4a2c-f9ca-74d2a1edd51c"
      },
      "outputs": [],
      "source": [
        "def parse_answer_key(answers):\n",
        "    answer_key = {}\n",
        "    for line in answers.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 3:\n",
        "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
        "            answer_key[question_number] = (correct_answer, skill)\n",
        "    return answer_key\n",
        "\n",
        "answers = ushistory_answers # Please update\n",
        "\n",
        "def grade_llm_answers(csv_filename, answer_key):\n",
        "    # Load the CSV file into a DataFrame\n",
        "    df = pd.read_csv(csv_filename)\n",
        "\n",
        "    # Add a new column for the truth (correct answer from the answer key)\n",
        "    df['Truth'] = \"\"\n",
        "\n",
        "    # Grading each response\n",
        "    for index, row in df.iterrows():\n",
        "        question_number = str(row['Question Number'])\n",
        "        # Retrieve the correct answer and skill from the answer key\n",
        "        if question_number in answer_key:\n",
        "            correct_answer, skill_assessed = answer_key[question_number]\n",
        "            df.at[index, 'Truth'] = correct_answer\n",
        "            df.at[index, 'Skill Assessed'] = skill_assessed\n",
        "\n",
        "            gpt_choice = extract_choice(str(row['GPT Choice']))\n",
        "            claude_choice = extract_choice(str(row['Claude Choice']))\n",
        "\n",
        "            # If GPT and Claude choices agree and are valid choices\n",
        "            if gpt_choice and claude_choice and gpt_choice == claude_choice:\n",
        "                df.at[index, 'Correctness'] = int(gpt_choice == correct_answer)\n",
        "            else:\n",
        "                # If either LLM choice is invalid or they disagree\n",
        "                df.at[index, 'Correctness'] = 0\n",
        "        else:\n",
        "            df.at[index, 'Correctness'] = 0\n",
        "            df.at[index, 'Skill Assessed'] = \"Question not in answer key\"\n",
        "\n",
        "    # Save the graded DataFrame back to a new CSV file\n",
        "    graded_filename = csv_filename.replace('Agree.csv', 'graded.csv')\n",
        "    df.to_csv(graded_filename, index=False)\n",
        "    print(f\"Graded CSV: {graded_filename}\")\n",
        "\n",
        "def extract_choice(choice_text):\n",
        "    # Extract letter choice from strings like \"1. C\" or \"5. C\"\n",
        "    match = re.match(r'\\d+\\.\\s*([A-E])', choice_text, re.IGNORECASE)\n",
        "    return match.group(1).upper() if match else None\n",
        "\n",
        "answer_key = parse_answer_key(answers)\n",
        "csv_filename = filename # Round_2_results\n",
        "grade_llm_answers(csv_filename, answer_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqUNNnUSFWla"
      },
      "source": [
        "#Round 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "32MIgeL7FYkQ",
        "outputId": "4a9378f4-f2d1-489f-fac0-018a9e82aaa5"
      },
      "outputs": [],
      "source": [
        "def parse_skills_assessed(answers):\n",
        "    skills_assessed = {}\n",
        "    for line in answers.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 3:\n",
        "            question_number = parts[0]\n",
        "            skill = ' '.join(parts[2:])\n",
        "            skills_assessed[question_number] = skill\n",
        "    return skills_assessed\n",
        "\n",
        "skills_assessed = parse_skills_assessed(answers)\n",
        "\n",
        "\n",
        "def parse_answer_key(answers):\n",
        "    answer_key = {}\n",
        "    for line in answers.splitlines():\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 3:\n",
        "            question_number, correct_answer, skill = parts[0], parts[1], ' '.join(parts[2:])\n",
        "            answer_key[question_number] = (correct_answer, skill)\n",
        "    return answer_key\n",
        "\n",
        "def extract_letter_choice(text):\n",
        "    match = re.search(r'\\b([A-E])\\b', text, re.IGNORECASE)\n",
        "    return match.group(1).upper() if match else None\n",
        "\n",
        "def initialize_csv_file(filename, subject):\n",
        "    with open(filename, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([f\"{subject}_Question Number\", f\"{subject}_Correctness\", f\"{subject}_Choice\", f\"{subject}_Skill Assessed\"])\n",
        "\n",
        "def update_csv(filename, answers, subject, answer_key):\n",
        "    with open(filename, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for i in range(1, 56):  # Add 1 more to the last question number to include it\n",
        "            answer_text = answers[i] if i in answers else \"\"\n",
        "            choice = extract_letter_choice(answer_text)\n",
        "            correct = 1 if answer_key.get(str(i), ('', ''))[0] == choice else 0\n",
        "            skill_assessed = answer_key.get(str(i), ('', ''))[1]\n",
        "            writer.writerow([f\"{subject}_{i}\", correct, choice, skill_assessed])\n",
        "\n",
        "def ask_gemini_all_questions(subject, filename, answer_key):\n",
        "    initialize_csv_file(filename, subject)\n",
        "    no_safety_settings = {\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        "    gemini_answers = {}\n",
        "\n",
        "    #i = 1 # Start from the first question\n",
        "\n",
        "    for i in range(1, 76):  # change based on last question\n",
        "        try:\n",
        "            response = model.generate_content(\n",
        "                contents=f\"Please answer the questions with only the question number and your letter of choice. Please remove parentheses from your letter of choice. Please answer according to the example format: '1. A' Now, please answer question {i}: \\n{question[i]}\\n based on GPT's reponse:'{Round_2_GPT_answer[i]}' \\nand Claude's response:'{Round_2_Claude_answer[i]}'. The percentage provided for their responses is their confidence level from 0% being least likely to be correct and 100% being most likelt to be the correct answer. This question assess the skill of: {skills_assessed[str(i)]} \\nYou are the judge and you can decide to choose which LLM's choice you want to choose. You can also decide to choose neither of their choices. Also decide based on each LLM's weaknesses:{LLM_Report}\",\n",
        "                safety_settings= no_safety_settings\n",
        "            )\n",
        "            gemini_answers[i] = response.text\n",
        "            print(response.text)\n",
        "        except Exception as e:  # Catching a general exception\n",
        "            error_message = str(e)\n",
        "            if \"429\" in error_message or \"rate limit\" in error_message.lower():\n",
        "                print(f\"Rate limit error on question {i}: {error_message} - Sleeping longer and retrying...\")\n",
        "                time.sleep(4)  # Sleep longer if rate limit error occurs\n",
        "            else:\n",
        "                print(f\"Error on question {i}: {error_message} - Retrying...\")\n",
        "        finally:\n",
        "            time.sleep(3)  # Respect the API's request limit\n",
        "\n",
        "    update_csv(filename, gemini_answers, subject, answer_key)\n",
        "    print(f\"Round 3 finalized. Results added to {filename}.\")\n",
        "\n",
        "filename = f\"Gemini_{subject}_Round_3_Results.csv\"\n",
        "answers = ushistory_answers\n",
        "answer_key = parse_answer_key(answers)\n",
        "ask_gemini_all_questions(subject, filename, answer_key)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
