{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4 Most poorly performed skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"GPT_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze poorly performed skills for each subject\n",
    "def analyze_poorly_performed_skills(df, subjects):\n",
    "    poorly_performed_skills = {}\n",
    "    for subject in subjects:\n",
    "        # Filter columns for the current subject\n",
    "        subject_cols = [col for col in df.columns if subject in col]\n",
    "        subject_df = df[subject_cols]\n",
    "\n",
    "        # Filter rows where correctness is 0\n",
    "        incorrect_answers_df = subject_df[subject_df[f\"{subject}_Correctness\"] == 0]\n",
    "\n",
    "        # Find the most frequently poorly performed skill\n",
    "        if not incorrect_answers_df.empty:\n",
    "            most_poorly_performed_skill = incorrect_answers_df[f\"{subject}_Skill_Assessed\"].value_counts().idxmax()\n",
    "            poorly_performed_skills[subject] = most_poorly_performed_skill\n",
    "        else:\n",
    "            poorly_performed_skills[subject] = \"No incorrect answers\"\n",
    "\n",
    "    return poorly_performed_skills\n",
    "\n",
    "# Define your subjects based on your column naming convention\n",
    "subjects = [\"US_History\", \"US_Government\", \"Physics\", \"Human_Geography\", \"Environmental_Science\"]\n",
    "\n",
    "# Analyze and print the poorly performed skills\n",
    "poorly_performed_skills = analyze_poorly_performed_skills(df, subjects)\n",
    "for subject, skill in poorly_performed_skills.items():\n",
    "    print(f\"{subject}: Most poorly performed skill - {skill}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4 Worst performed skill across all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"GPT_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze the worst performed skill across all subjects\n",
    "def analyze_worst_performed_skill_with_details(df):\n",
    "    # Extracting all skill assessed and correctness columns for analysis\n",
    "    skill_columns = [col for col in df.columns if \"Skill_Assessed\" in col]\n",
    "    correctness_columns = [col.replace(\"Skill_Assessed\", \"Correctness\") for col in skill_columns]\n",
    "\n",
    "    # Creating a DataFrame for skills and their correctness\n",
    "    skills_df = pd.DataFrame()\n",
    "    for skill_col, correctness_col in zip(skill_columns, correctness_columns):\n",
    "        temp_df = df[[skill_col, correctness_col]].rename(columns={skill_col: \"Skill\", correctness_col: \"Correctness\"})\n",
    "        skills_df = pd.concat([skills_df, temp_df])\n",
    "\n",
    "    # Filter incorrect answers\n",
    "    incorrect_skills_df = skills_df[skills_df[\"Correctness\"] == 0]\n",
    "    total_incorrect_answers = len(incorrect_skills_df)\n",
    "\n",
    "    # Find the most frequently poorly performed skill\n",
    "    if not incorrect_skills_df.empty:\n",
    "        worst_performed_skill = incorrect_skills_df[\"Skill\"].value_counts().idxmax()\n",
    "        count = incorrect_skills_df[\"Skill\"].value_counts().max()\n",
    "        percentage = (count / total_incorrect_answers) * 100\n",
    "    else:\n",
    "        worst_performed_skill = \"No incorrect answers\"\n",
    "        count = 0\n",
    "        percentage = 0\n",
    "\n",
    "    return worst_performed_skill, count, total_incorrect_answers, percentage\n",
    "\n",
    "# Analyze and print the worst performed skill with details\n",
    "worst_skill, count, total_incorrect, percentage = analyze_worst_performed_skill_with_details(df)\n",
    "print(f\"Worst performed skill across all subjects: {worst_skill}\")\n",
    "print(f\"Times answered incorrectly: {count}\")\n",
    "print(f\"Total incorrect answers across all subjects: {total_incorrect}\")\n",
    "print(f\"Percentage of all incorrect answers: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Pro Most poorly performed skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"Gemini_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze poorly performed skills for each subject\n",
    "def analyze_poorly_performed_skills(df, subjects):\n",
    "    poorly_performed_skills = {}\n",
    "    for subject in subjects:\n",
    "        # Filter columns for the current subject\n",
    "        subject_cols = [col for col in df.columns if subject in col]\n",
    "        subject_df = df[subject_cols]\n",
    "\n",
    "        # Filter rows where correctness is 0\n",
    "        incorrect_answers_df = subject_df[subject_df[f\"{subject}_Correctness\"] == 0]\n",
    "\n",
    "        # Find the most frequently poorly performed skill\n",
    "        if not incorrect_answers_df.empty:\n",
    "            most_poorly_performed_skill = incorrect_answers_df[f\"{subject}_Skill_Assessed\"].value_counts().idxmax()\n",
    "            poorly_performed_skills[subject] = most_poorly_performed_skill\n",
    "        else:\n",
    "            poorly_performed_skills[subject] = \"No incorrect answers\"\n",
    "\n",
    "    return poorly_performed_skills\n",
    "\n",
    "# Define your subjects based on your column naming convention\n",
    "subjects = [\"US_History\", \"US_Government\", \"Physics\", \"Human_Geography\", \"Environmental_Science\"]\n",
    "\n",
    "# Analyze and print the poorly performed skills\n",
    "poorly_performed_skills = analyze_poorly_performed_skills(df, subjects)\n",
    "for subject, skill in poorly_performed_skills.items():\n",
    "    print(f\"{subject}: Most poorly performed skill - {skill}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Pro Worst performed skill across all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"Gemini_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze the worst performed skill across all subjects\n",
    "def analyze_worst_performed_skill_with_details(df):\n",
    "    # Extracting all skill assessed and correctness columns for analysis\n",
    "    skill_columns = [col for col in df.columns if \"Skill_Assessed\" in col]\n",
    "    correctness_columns = [col.replace(\"Skill_Assessed\", \"Correctness\") for col in skill_columns]\n",
    "\n",
    "    # Creating a DataFrame for skills and their correctness\n",
    "    skills_df = pd.DataFrame()\n",
    "    for skill_col, correctness_col in zip(skill_columns, correctness_columns):\n",
    "        temp_df = df[[skill_col, correctness_col]].rename(columns={skill_col: \"Skill\", correctness_col: \"Correctness\"})\n",
    "        skills_df = pd.concat([skills_df, temp_df])\n",
    "\n",
    "    # Filter incorrect answers\n",
    "    incorrect_skills_df = skills_df[skills_df[\"Correctness\"] == 0]\n",
    "    total_incorrect_answers = len(incorrect_skills_df)\n",
    "\n",
    "    # Find the most frequently poorly performed skill\n",
    "    if not incorrect_skills_df.empty:\n",
    "        worst_performed_skill = incorrect_skills_df[\"Skill\"].value_counts().idxmax()\n",
    "        count = incorrect_skills_df[\"Skill\"].value_counts().max()\n",
    "        percentage = (count / total_incorrect_answers) * 100\n",
    "    else:\n",
    "        worst_performed_skill = \"No incorrect answers\"\n",
    "        count = 0\n",
    "        percentage = 0\n",
    "\n",
    "    return worst_performed_skill, count, total_incorrect_answers, percentage\n",
    "\n",
    "# Analyze and print the worst performed skill with details\n",
    "worst_skill, count, total_incorrect, percentage = analyze_worst_performed_skill_with_details(df)\n",
    "print(f\"Worst performed skill across all subjects: {worst_skill}\")\n",
    "print(f\"Times answered incorrectly: {count}\")\n",
    "print(f\"Total incorrect answers across all subjects: {total_incorrect}\")\n",
    "print(f\"Percentage of all incorrect answers: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calude 3 Opus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3 Opus Pro Most poorly performed skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"Claude_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze poorly performed skills for each subject\n",
    "def analyze_poorly_performed_skills(df, subjects):\n",
    "    poorly_performed_skills = {}\n",
    "    for subject in subjects:\n",
    "        # Filter columns for the current subject\n",
    "        subject_cols = [col for col in df.columns if subject in col]\n",
    "        subject_df = df[subject_cols]\n",
    "\n",
    "        # Filter rows where correctness is 0\n",
    "        incorrect_answers_df = subject_df[subject_df[f\"{subject}_Correctness\"] == 0]\n",
    "\n",
    "        # Find the most frequently poorly performed skill\n",
    "        if not incorrect_answers_df.empty:\n",
    "            most_poorly_performed_skill = incorrect_answers_df[f\"{subject}_Skill_Assessed\"].value_counts().idxmax()\n",
    "            poorly_performed_skills[subject] = most_poorly_performed_skill\n",
    "        else:\n",
    "            poorly_performed_skills[subject] = \"No incorrect answers\"\n",
    "\n",
    "    return poorly_performed_skills\n",
    "\n",
    "# Define your subjects based on your column naming convention\n",
    "subjects = [\"US_History\", \"US_Government\", \"Physics\", \"Human_Geography\", \"Environmental_Science\"]\n",
    "\n",
    "# Analyze and print the poorly performed skills\n",
    "poorly_performed_skills = analyze_poorly_performed_skills(df, subjects)\n",
    "for subject, skill in poorly_performed_skills.items():\n",
    "    print(f\"{subject}: Most poorly performed skill - {skill}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude 3 Opus Worst performed skill across all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"Claude_Results.xlsx\", engine='openpyxl')\n",
    "\n",
    "# Rename the columns appropriately\n",
    "new_columns = [\n",
    "    'US_History_Question_Number', 'US_History_Correctness', 'US_History_Skill_Assessed', 'US_History_Time_Taken',\n",
    "    'US_Government_Question_Number', 'US_Government_Correctness', 'US_Government_Skill_Assessed', 'US_Government_Time_Taken',\n",
    "    'Physics_Question_Number', 'Physics_Correctness', 'Physics_Skill_Assessed', 'Physics_Time_Taken',\n",
    "    'Human_Geography_Question_Number', 'Human_Geography_Correctness', 'Human_Geography_Skill_Assessed', 'Human_Geography_Time_Taken',\n",
    "    'Environmental_Science_Question_Number', 'Environmental_Science_Correctness', 'Environmental_Science_Skill_Assessed', 'Environmental_Science_Time_Taken'\n",
    "]\n",
    "df.columns = new_columns\n",
    "\n",
    "# Function to analyze the worst performed skill across all subjects\n",
    "def analyze_worst_performed_skill_with_details(df):\n",
    "    # Extracting all skill assessed and correctness columns for analysis\n",
    "    skill_columns = [col for col in df.columns if \"Skill_Assessed\" in col]\n",
    "    correctness_columns = [col.replace(\"Skill_Assessed\", \"Correctness\") for col in skill_columns]\n",
    "\n",
    "    # Creating a DataFrame for skills and their correctness\n",
    "    skills_df = pd.DataFrame()\n",
    "    for skill_col, correctness_col in zip(skill_columns, correctness_columns):\n",
    "        temp_df = df[[skill_col, correctness_col]].rename(columns={skill_col: \"Skill\", correctness_col: \"Correctness\"})\n",
    "        skills_df = pd.concat([skills_df, temp_df])\n",
    "\n",
    "    # Filter incorrect answers\n",
    "    incorrect_skills_df = skills_df[skills_df[\"Correctness\"] == 0]\n",
    "    total_incorrect_answers = len(incorrect_skills_df)\n",
    "\n",
    "    # Find the most frequently poorly performed skill\n",
    "    if not incorrect_skills_df.empty:\n",
    "        worst_performed_skill = incorrect_skills_df[\"Skill\"].value_counts().idxmax()\n",
    "        count = incorrect_skills_df[\"Skill\"].value_counts().max()\n",
    "        percentage = (count / total_incorrect_answers) * 100\n",
    "    else:\n",
    "        worst_performed_skill = \"No incorrect answers\"\n",
    "        count = 0\n",
    "        percentage = 0\n",
    "\n",
    "    return worst_performed_skill, count, total_incorrect_answers, percentage\n",
    "\n",
    "# Analyze and print the worst performed skill with details\n",
    "worst_skill, count, total_incorrect, percentage = analyze_worst_performed_skill_with_details(df)\n",
    "print(f\"Worst performed skill across all subjects: {worst_skill}\")\n",
    "print(f\"Times answered incorrectly: {count}\")\n",
    "print(f\"Total incorrect answers across all subjects: {total_incorrect}\")\n",
    "print(f\"Percentage of all incorrect answers: {percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
